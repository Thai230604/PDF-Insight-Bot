import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
import os
from dotenv import load_dotenv

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

st.header("üëæChatbot H·ªèi ƒê√°p T√†i Li·ªáu PDF")

with st.sidebar:
    st.title("T·∫£i T√†i Li·ªáu")
    file = st.file_uploader("T·∫£i l√™n file PDF ƒë·ªÉ b·∫Øt ƒë·∫ßu h·ªèi ƒë√°p", type="pdf")

# ƒê·ªãnh nghƒ©a prompt t·ªëi ∆∞u cho ti·∫øng Vi·ªát
prompt_template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, am hi·ªÉu vƒÉn h√≥a v√† ng√¥n ng·ªØ Vi·ªát Nam. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p. 
T·∫•t c·∫£ c√¢u tr·∫£ l·ªùi ph·∫£i ƒë∆∞·ª£c vi·∫øt b·∫±ng ti·∫øng Vi·ªát, s·ª≠ d·ª•ng ng√¥n ng·ªØ t·ª± nhi√™n, d·ªÖ hi·ªÉu, v√† ph√π h·ª£p v·ªõi ng·ªØ c·∫£nh Vi·ªát Nam. 
N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu, h√£y n√≥i r√µ r·∫±ng th√¥ng tin kh√¥ng c√≥ v√† cung c·∫•p c√¢u tr·∫£ l·ªùi h·ª£p l√Ω d·ª±a tr√™n ki·∫øn th·ª©c chung.

C√¢u h·ªèi: {question}
N·ªôi dung t√†i li·ªáu: {context}
Tr·∫£ l·ªùi:
"""

prompt = PromptTemplate(template=prompt_template, input_variables=["question", "context"])

if file is not None:
    # ƒê·ªçc n·ªôi dung PDF
    pdf_reader = PdfReader(file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text() or ""  # X·ª≠ l√Ω trang kh√¥ng c√≥ vƒÉn b·∫£n

    # Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n
    text_splitter = RecursiveCharacterTextSplitter(
        separators="\n",
        chunk_size=1000,
        chunk_overlap=150,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    st.write(chunks)  # B·ªè comment n·∫øu mu·ªën hi·ªÉn th·ªã c√°c ƒëo·∫°n vƒÉn b·∫£n

    # S·ª≠ d·ª•ng m√¥ h√¨nh nh√∫ng ti·∫øng Vi·ªát
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")


    # T·∫°o vector store v·ªõi FAISS
    vector_store = FAISS.from_texts(chunks, embeddings)

    # Kh·ªüi t·∫°o DeepSeek API client
    llm = ChatOpenAI(
        openai_api_key=DEEPSEEK_API_KEY,
        openai_api_base="https://api.deepseek.com/v1",  # base_url c·ªßa DeepSeek
        model_name="deepseek-chat",  # ho·∫∑c t√™n m√¥ h√¨nh c·ª• th·ªÉ c·ªßa DeepSeek
        temperature=0.7
)

    # T·∫°o chu·ªói truy v·∫•n v·ªõi DeepSeek API
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(),
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": prompt}
)

    # Nh·∫≠p c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
    user_question = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n t·∫°i ƒë√¢y")

    if user_question:
        # Th√™m h∆∞·ªõng d·∫´n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
        user_question = f"H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát: {user_question}"
        # Th·ª±c hi·ªán truy v·∫•n
        response = chain({"question": user_question, "chat_history": []})
        st.write(response["answer"])  # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi